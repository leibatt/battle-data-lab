I"±9<h1 id="projects">Projects</h1>

<h2 id="ongoing-projects">Ongoing Projects</h2>

<div class="well">
  <pubtit><h4><b>Data Prefetching for Interactive Multi-user environments</b></h4></pubtit>

  <p><em>Fall 2019 ~ Current</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Ameya Patil, Junran Yang, Sai Yerramreddy, Abhilasha Sancheti, Leilani Battle</b>, Remco Chang, Michael Stonebraker</em></p>

  <p>Interactive data visualization and analysis tools have greatlyfacilitated the process of understanding and extracting in-sights from any kind of dataset. These tools usually makeuse of a client-server architecture where the data is storedon a DBMS and is queried by the user via a client ma-chine. Such a design also allows collaborative concurrentdata exploration among multiple users using the system.Some queries are very complex to return results at inter-active speeds and DBMSs are not usually designed forresponding to user queries at interactive speeds. Thiscan introduce latency in the tool and hamper the data ex-ploration experience. To facilitate a low-latency queryresponse, we built a middleware layer between the DBMSand the client which exploits possible overlaps between thequeries of multiple concurrent user sessions, with efficientcaching strategies to control data retrieval and storage forall the users. We also built a prediction engine to predictthe next action of any user and pre-fetch the required databefore hand. We propose and evaluate the use of neural net-works for building a prediction engine and designs for a pre-fetch tiles manager unit to handle the pre-fetched tiles forboth single user and concurrent multi-user exploration sce-narios. We present results using our implementation whichshow that our system helps in having fast interaction speed,efficient usage of main memory and thus better user experience.</p>
</div>

<div class="well">
  <pubtit><h4><b>Performance Driven UI Design</b></h4></pubtit>

  <p><em>Summer 2019 ~ Current</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Joanna Purich, Urisha Kreem, Leilani Battle</b></em></p>

  <p><b>Past Members:</b> <em>Chidi Udeze, Shriya Sharma</em></p>

  <p>Interactive analysis systems provide efficient and accessible means by which users of varying technical experience can comfortably manipulate and analyze data using interactive widgets. Widgets are elements of interaction within a user interface (e.g. scrollbar, button, etc). Interactions with these widgets produce database queries whose results determine the subsequent changes made to the current visualization made by the user. In this paper, we present a tool that extends IDEBench to ingest visualization interfaces and a dataset, and estimate the expected database load that would be generated by real users. Our tool analyzes the interactive capabilities of the visualization and creates the queries that support the various interactions. We began with a proof of concept implementation of every interaction widget, which led us to define three distinct sets of query templates that can support all interactions. We then show that these templates can be layered to imitate various interfaces and tailored to any dataset. Secondly, we simulate how users would interact with the proposed interface and report on the strain that such use would place on the database management system.</p>
</div>

<div class="well">
  <pubtit><h4><b>Code Code Evolution</b></h4></pubtit>

  <p><img src="http://localhost:4000/images/projpic/code-code-evolution.png" class="img-responsive" width="45%" style="float: left" /></p>

  <p><em>Summer 2019 ~ Current</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Deepthi Raghunandan, Leilani Battle</b>, Niklas Elmqvist</em></p>

  <p><b>Past Members:</b> <em>Rachael Zehrung</em></p>

  <p>The advent of open source computational notebooks has changed the landscape for analysts looking to document visualizations, code, and annotate various stages of the analysis process under one platform. The use of GitHub in conjunction with computational notebooks provides a unique opportunity to study the data analysis process over time. We present a threepart quantitative study that examines process-based data science workflows in the context of Jupyter notebooks. The first study demonstrates how to identify correlations between a userâ€™s data analysis behavior and observable GitHub interactions. The second study establishes a methodology by which data science workflows can be characterized within notebooks: in particular, exploration and explanation. In the last study, we apply this methodology to investigate shifts in workflows across time. All three parts examine the evolving role of visualization from exploration and analysis to communication and documentation. We use these observations as an opportunity to encourage a design/re-design of data science and visualization tools to better support process-based workflows.</p>
</div>

<div class="well">
  <pubtit><h4><b>Vis Ex Machina&#58; An Analysis of Trust in Human versus Algorithmically Generated Visualization Recommendations</b></h4></pubtit>

  <p><img src="http://localhost:4000/images/projpic/vis-ex-machina.png" class="img-responsive" width="45%" style="float: left" /></p>

  <p><em>Summer 2019 ~ Current</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Astha Singhal, Sarah Agarrat</b>, Michael Correll, <b style="color:#cc3300">Leilani Battle</b></em></p>

  <p><b>Past Members:</b> <em>Rachael Zehrung</em></p>

  <p>More visualization systems are simplifying the data analysis process by automatically suggesting relevant visualizations. However, little work has been done to understand if users trust these automated recommendations. In this paper, we present the results of a crowd-sourced study exploring preferences and perceived quality of recommendations that have been positioned as either human-curated or algorithmically generated. We observe that while participants initially prefer human recommenders, their actions suggest an indifference for recommendation source when evaluating visualization recommendations. The relevance of presented information (e.g., the presence of certain data fields) was the most critical factor, followed by a belief in recommenderâ€™s ability to create accurate visualizations. Our findings suggest a general indifference towards the provenance of recommendations, and point to idiosyncratic definitions of visualization quality and trustworthiness that may not be captured by simple measures. We suggest that recommendation systems should be tailored to the information-foraging strategies of specific users.</p>
</div>

<div class="well">
  <pubtit><h4><b>Lodestar</b></h4></pubtit>

  <p><img src="http://localhost:4000/images/projpic/vis-ex-machina.png" class="img-responsive" width="45%" style="float: left" /></p>

  <p><em>Spring 2019 ~ Current</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Deepthi Raghunandan Leilani Battle</b>, Niklas Elmqvist, <b style="color:#cc3300">Tejaswi Shrestha, Kartik Krishnan, Segen Tirfe, Shenzhi Shi</b></em></p>

  <p><b>Past Members:</b> <em>Rachael Zehrung, Jason Lim, Zhe Cui</em></p>

  <p>Keeping abreast of current trends, technologies, and best practices in statistics, visualization, and data analysis is becoming increasingly difficult even for professional data scientists, and is a hopeless endeavor for domain experts lacking time and training. In this paper, we propose Lodestar, an interactive visualization sandbox that allows users to perform visual data analysis simply by selecting from a list of recommendations. Choosing a recommendation adds the corresponding Python code to the notebook and executes it, thus generating new output. The recommendation engine is inspired by autocomplete mechanisms, where a partial query is used to show suggestions for how to finish it. In our implementation, we derive our recommendations from a directed graph of analysis states: one manually curated from online data science tutorials, another by automatically analyzing the Python code from a corpus of approximately 6,000 Jupyter notebooks on data science. We evaluated Lodestar through a two-phase evaluation sequence: a formative study guiding our next set of improvements to the tool, followed by a summative study assessing its utility for novice data scientists.</p>
</div>

<div class="well">
  <pubtit><h4><b>Beagle&#58; Interactive Visualizations</b></h4></pubtit>

  <p><img src="http://localhost:4000/images/projpic/beagle.png" class="img-responsive" width="45%" style="float: left" /></p>

  <p><em>Spring 2018 ~ Current</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Hannah Bako, Alisha Varma, Anu Faboro, Mahreen Haider, Leilani Battle</b></em></p>

  <p><b>Past Members:</b> <em>Arjun Nair, Kelli Webber, Rishik Narayana, Danni Feng</em></p>

  <p><span><a href="http://www.cs.umd.edu/~leilani/static/papers/CHI2018_cr_battle_02_05_2018.pdf" target="_blank">Publication - 2018 CHI Conference, Pages 1-8</a></span></p>

  <p><span><a href="https://youtu.be/Nv2CNx5xKkM" target="_blank">Video (preview)</a></span></p>

  <p>With the adoption of visualizations as a means of communication in various industries, thousands of visualizations have been created and are available of the World Wide Web. The Beagle Projectâ€™s focus is on analyzing these visualizations to 1) understand how these visualizations are created and applied. 2) Understand the challenges involved in creating these visualizations. 3) To assess how users goals can be supported through automated tools. Over the years, there have been 3 phases of this project. The first phase aimed to quantify and assess how interactive visualizations are used by visualization architects on the web. The Beagle tool extracted SVG visualizations from popular visualization sharing websites. It exploits the unique positioning and characteristics of visualizations marks to extract features and couples this with Machine Learning techniques to label these visualizations with a 97% accuracy. Given the popularity of D3 visualizations among the extracted SVGâ€™s analyzed in the first project, the second phase focused on analyzing how D3 users reason about and use its framework. This was done by assessing how users describe their challenges and reasoning process on multiple online communities such as Stackoverflow and Reddit.<br /> The results of these two phases highlighted both the dominance of D3.js as a tool for the creation of complex and interactive visualizations as well as its steep learning curve.  Past research has also highlighted a culture of visualization architects modifying existing examples to create their own visualizations. The current phase of the Beagle project leverages all of these findings to analyze a corpus of D3 visualizations scrapped from Bl.ocks.org in order to understand what programming techniques visualization architects use in creating these visualizations. We aim to strike a balance between automated guidance and user agency by first deriving a model of  how visualization designers use manual specification languagesâ€“specifically D3â€“to develop new visualization designs. Subsequently, we explore the application of this model in an online tool that assists developers to create visualizations via informed recommendations. Finally, we apply programming language theory to effect the direct manipulation and fitting of a users code based on their specified design goals.</p>
</div>

<h2 id="past-projects">Past Projects</h2>

<div class="well">
  <pubtit><h4><b>A Model and Application for Cross-domain Visualization System Design</b></h4></pubtit>

  <p><img src="http://localhost:4000/images/projpic/cross-domian-visualization-system-design.png" class="img-responsive" width="45%" style="float: left" /></p>

  <p><em>Spring 2019 ~ Summer 2020</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Sneha Gathani</b>, Daniel Votipka, Kartik Krishnan, Kristopher Micinski, Jeffrey Foster, Michelle Mazurek, <b style="color:#cc3300">Leilani Battle</b></em></p>

  <p>Visualization design studies are notoriously difficult to design effectively. Though existing models highlight the major pitfalls, their guidance is not as user-friendly for individuals new to design studies. We present an updated design study model providing step-by-step guidelines, concrete examples, and discussion of differences and similarities between design studies in eight different domains.  To demonstrate the value of our model and guidelines, we apply them in the security domain to help fledgling analysts reverse engineer (RE) Android applications (apps) for potential security and privacy vulnerabilities. Through our design study, we develop TraceInspector, an interactive visualization tool that integrates both static and dynamic Android app data, connects relevant temporal event sequences and method dependencies, and executes app code in a single visualization interface. Finally, we evaluate TraceInspector with nine RE users and find that the tool eases the learning of RE tasks for novice RE users, validating our synthesized design study guidance.</p>
</div>

<div class="well">
  <pubtit><h4><b>Debugging Database Queries&#58; A Survey of Tools, Techniques, and Users</b></h4></pubtit>

  <p><em>Spring 2019 ~ Fall 2019</em></p>
  <p><b>Members:</b> <em><b style="color:#cc3300">Sneha Gathani, Peter Lim, Leilani Battle</b></em></p>

  <p><span><a href="https://www.cs.umd.edu/~leilani/static/papers/gathani_debugging_CHI_2020.pdf" target="_blank">Publication - 2020 CHI Conference, Pages 1-16</a></span></p>

  <p><span><a href="https://www.youtube.com/watch?v=auCLjsBi6zE" target="_blank">Video (presentation)</a></span></p>

  <p>Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualization and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between usersâ€™ debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.</p>
</div>

:ET